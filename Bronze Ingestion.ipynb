{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f16385e-b39e-4cba-a311-3588ddf8ef61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze layer\n",
    "\n",
    "The bronze layer contains raw data. It is intended to serves as the single source of truth, preserving the dataâ€™s fidelity, and enables reprocessing and auditing by retaining all historical data.\n",
    "\n",
    "In the bronze layer we are going to add field to the tables:\n",
    "* Ingestion_Time\n",
    "* Hash - to map row's changes\n",
    "* State - addopt the operation made in SQL database (like the cdc code)\n",
    "\n",
    "| **Operation** | **CDC Code** |  **Comments** |\n",
    "|--------------|------------|------------|\n",
    "| Delete   | `1`        | implemented with MERGE |\n",
    "| Insert   | `2`        | implemented with MERGE |\n",
    "| Update (before image) | `3` | implemented with MERGE |\n",
    "| Update (after image)  | `4` | implemented with INSERT after MERGE |\n",
    "\n",
    "Traditionally, Delta Lake offered a **_MERGE INTO_** command to capture changes. **_MERGE INTO_** has a warning point related of the order of change data. Since we reading data directly from a SQL database, we just read a state of database. So, we are not woried about change orders.\n",
    "\n",
    "We are declaring tables in a lazy way (not declaring all columns). Doing that, we are already take advantage of schema evolution.\n",
    "\n",
    "* For now, we are just taking the Customer table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be2abf8-f7f4-423d-aaeb-b969dcab289d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5dd3c4-a584-46ca-9b3f-a32849217a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "from pyspark.sql.functions import col, concat_ws, sha2, lit #current_timestamp, from_utc_timestamp, \n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5806020e-a7d0-4ee6-a70d-282e63675788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Credentials for Azure SQL database (using Azure Key Vault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656efa45-e68e-49ed-bd7c-b38a6036bde5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the credentials of Azure SQL database\n",
    "sql_db_retail_key = dbutils.secrets.get(scope=\"keys\", key=\"sqldbretailkey\")\n",
    "sql_db_retail_url = \"jdbc:sqlserver://retail-oltp-server.database.windows.net:1433;databaseName=retail\"\n",
    "sql_db_retail_user = \"andre\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.retail_url\", sql_db_retail_url)\n",
    "spark.conf.set(\"spark.sql.retail_user\", sql_db_retail_user)\n",
    "spark.conf.set(\"spark.sql.retail_key\", sql_db_retail_key)\n",
    "\n",
    "properties = {\n",
    "    \"user\": sql_db_retail_user,         # Replace with your username\n",
    "    \"password\": sql_db_retail_key,      # Replace with your password\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "print(properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e3d475-622a-4847-90a7-bad40917d817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Defining what tables to ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67876af-efee-442d-a171-fbca977114a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary with tables and their primary keys\n",
    "primary_keys = {\n",
    "    \"CUSTOMERS\": [\"CUSTOMER_ID\"] #,\n",
    "#    \"BRANDS\": [\"BRAND_ID\"],\n",
    "#    \"CATEGORIES\": [\"CATEGORY_ID\"],\n",
    "#    \"PRODUCTS\": [\"PRODUCT_ID\"],\n",
    "#    \"STORES\": [\"STORE_ID\"],\n",
    "#    \"PROMOTIONS\": [\"PROMOTION_ID\"],\n",
    "#    \"PAYMENT_METHODS\": [\"PAYMENT_METHOD_ID\"],\n",
    "#    \"INVENTORY\": [\"INVENTORY_ID\"],\n",
    "#    \"SALES\": [\"SALE_ID\"],\n",
    "#    \"TRANSACTION_ITEM\": [\"TRANSACTION_ID\"] \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c8e3ae-1499-423a-99bb-ca51ced7c0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c688c93c-611c-4cdd-8df3-93da3cc39aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in primary_keys:\n",
    "    # Get the list of primary keys for the current table\n",
    "    keys = primary_keys[table]\n",
    "\n",
    "    # Convert the list of primary keys into a SQL column definition\n",
    "    primary_keys_str = \",\\n  \".join([f\"{key} INTEGER\" for key in keys]) # DEALING WITH COMPOSITE KEYS (just integers)\n",
    "    merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in keys])\n",
    "    ids = \", \".join([f\"{key}\" for key in keys])\n",
    "\n",
    "    # Build condition for merge\n",
    "    merge_condition = \" AND \".join([f\"target.{key} = source.{key}\" for key in keys])       # source.IDs = target.IDs\n",
    "\n",
    "    # freezed timestamp:\n",
    "    freezed_timestamp_dt = datetime.now(pytz.timezone(\"America/Sao_Paulo\"))\n",
    "    freezed_timestamp = lit(freezed_timestamp_dt)\n",
    "\n",
    "    # taking the bronze table inside delta lake\n",
    "    df_bronze = DeltaTable.forName(spark, f\"bronze.{table}\")\n",
    "\n",
    "    # reading the source table\n",
    "    df_source = spark.read \\\n",
    "        .jdbc(\n",
    "            url=sql_db_retail_url,\n",
    "            table=f\"dbo.{table}\",\n",
    "            properties=properties)\n",
    "\n",
    "    # Add basics collumns to the source DataFrame:\n",
    "    df_source = df_source.withColumn(\"hash\", sha2(concat_ws(\"|\", *[col(c) for c in df_source.columns]), 256)) \n",
    "    df_source = df_source.withColumn(\"load_timestamp\", freezed_timestamp)\n",
    "    df_source = df_source.withColumn(\"end_timestamp\", lit(None).cast(\"timestamp\"))\n",
    "    df_source = df_source.withColumn(\"state\", lit(2))   \n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "    #################################### Start Bronze ingestion ####################################\n",
    "\n",
    "    #       | **Operation**         | **CDC Code** |\n",
    "    #       |-----------------------|--------------|\n",
    "    #       | Delete                |     `1`      |\n",
    "    #       | Insert                |     `2`      |\n",
    "    #       | Update (before image) |     `3`      |\n",
    "    #       | Update (after image)  |     `4`      |   => inserting outside the merge\n",
    "\n",
    "\n",
    "    # Mark 1 for delete and 3 for update (before image) in bronze.table\n",
    "    # Insert: STATE = 2 \n",
    "    df_bronze.alias(\"target\").merge(\n",
    "        df_source.alias(\"source\"),\n",
    "        merge_condition + \" AND target.state in (2,4)\"   # source.IDs = target.IDs\n",
    "    ).withSchemaEvolution(\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"target.hash != source.hash AND target.state in (2,4)\",\n",
    "        set={\n",
    "            \"state\": \"3\",\n",
    "            \"end_timestamp\": freezed_timestamp\n",
    "        }\n",
    "    ).whenNotMatchedBySourceUpdate(\n",
    "        condition=\"target.state in (2,4)\",\n",
    "        set={\n",
    "            \"state\": \"1\",\n",
    "            \"end_timestamp\": freezed_timestamp\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll(\n",
    "    ).execute()\n",
    "\n",
    "    # Filtering all Update (after image) STATE = 4:\n",
    "    df_source_state4 = df_source \\\n",
    "        .withColumn(\"state\", lit(4)) \\\n",
    "        .join(\n",
    "            df_bronze.toDF() \\\n",
    "                .filter((col(\"state\") == 3) & (col(\"end_timestamp\") == freezed_timestamp_dt)) \\\n",
    "                .select(*keys),  \n",
    "            on=keys,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Inserting all Update (after image) STATE = 4:\n",
    "    df_source_state4.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"bronze.{table}\")\n",
    "    \n",
    "    print(f\"Table bronze.{table} updated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 559752049615868,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
